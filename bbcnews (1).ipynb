{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Presentado por:\n",
        "Natalia Rubio\n",
        "Cristian Gonzalez"
      ],
      "metadata": {
        "id": "KbKaxHnBs3io"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# %% [1. Instalaci√≥n de dependencias]\n",
        "!pip install -q transformers==4.51.3 datasets==2.14.4 torch==2.6.0 ipywidgets==7.7.1\n",
        "\n",
        "# %% [2. Montar Google Drive]\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "# %% [3. Configurar rutas]\n",
        "import os\n",
        "base_path = '/content/drive/MyDrive/BBC News Summary'\n",
        "articles_dir = os.path.join(base_path, 'News Articles')\n",
        "model_save_path = '/content/drive/MyDrive/News_Generator_Final'\n",
        "os.makedirs(model_save_path, exist_ok=True)\n",
        "\n",
        "# %% [4. Cargar y procesar datos]\n",
        "import pandas as pd\n",
        "import glob\n",
        "from datetime import datetime\n",
        "\n",
        "def cargar_articulos(categoria):\n",
        "    articulos = []\n",
        "    for archivo in glob.glob(os.path.join(articles_dir, categoria, '*.txt')):\n",
        "        with open(archivo, 'r', encoding='latin-1') as f:\n",
        "            contenido = f.read().split('\\n')\n",
        "            titulo = contenido[0].strip()\n",
        "            cuerpo = ' '.join([linea.strip() for linea in contenido[1:] if linea.strip()])\n",
        "            articulos.append({\n",
        "                'category': categoria.upper(),\n",
        "                'title': titulo,\n",
        "                'content': cuerpo[:1800]  # Limitar longitud\n",
        "            })\n",
        "    return articulos\n",
        "\n",
        "categorias = ['business', 'entertainment', 'politics', 'sport', 'tech']\n",
        "df = pd.DataFrame([art for cat in categorias for art in cargar_articulos(cat)])\n",
        "\n",
        "# Formatear texto con estructura mejorada\n",
        "df['text'] = df.apply(\n",
        "    lambda x: f\"\"\"\\\n",
        "[{x['category']}]\n",
        "FECHA: {datetime.now().strftime('%Y-%m-%d')}\n",
        "TITULAR: {x['title']}\n",
        "CONTENIDO: {x['content']}\n",
        "---\"\"\",\n",
        "    axis=1\n",
        ")\n",
        "\n",
        "# %% [5. Tokenizaci√≥n]\n",
        "from transformers import GPT2Tokenizer\n",
        "\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('distilgpt2')\n",
        "tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
        "\n",
        "def funcion_tokenizacion(ejemplos):\n",
        "    return tokenizer(\n",
        "        ejemplos['text'],\n",
        "        max_length=512,\n",
        "        padding='max_length',\n",
        "        truncation=True,\n",
        "        return_tensors='pt'\n",
        "    )\n",
        "\n",
        "from datasets import Dataset\n",
        "dataset = Dataset.from_pandas(df[['text']])\n",
        "dataset = dataset.map(\n",
        "    funcion_tokenizacion,\n",
        "    batched=True,\n",
        "    batch_size=32,\n",
        "    remove_columns=['text']\n",
        ")\n",
        "\n",
        "# Divisi√≥n train/validation (90/10)\n",
        "dataset = dataset.train_test_split(test_size=0.1)\n",
        "\n",
        "# %% [6. Configurar modelo y entrenamiento]\n",
        "from transformers import GPT2LMHeadModel, TrainingArguments, Trainer, DataCollatorForLanguageModeling\n",
        "\n",
        "modelo = GPT2LMHeadModel.from_pretrained('distilgpt2')\n",
        "modelo.resize_token_embeddings(len(tokenizer))\n",
        "\n",
        "data_collator = DataCollatorForLanguageModeling(\n",
        "    tokenizer=tokenizer,\n",
        "    mlm=False\n",
        ")\n",
        "\n",
        "# Hiperpar√°metros optimizados para versi√≥n 4.51.3\n",
        "argumentos_entrenamiento = TrainingArguments(\n",
        "    output_dir='./resultados',\n",
        "    num_train_epochs=10,\n",
        "    per_device_train_batch_size=4,\n",
        "    per_device_eval_batch_size=4,\n",
        "    gradient_accumulation_steps=2,\n",
        "    eval_steps=500,  # Evaluar cada 500 pasos\n",
        "    save_steps=500,  # Guardar cada 500 pasos\n",
        "    learning_rate=2e-5,\n",
        "    weight_decay=0.01,\n",
        "    fp16=True,\n",
        "    report_to=\"none\"\n",
        ")\n",
        "\n",
        "entrenador = Trainer(\n",
        "    model=modelo,\n",
        "    args=argumentos_entrenamiento,\n",
        "    train_dataset=dataset[\"train\"],\n",
        "    eval_dataset=dataset[\"test\"],\n",
        "    data_collator=data_collator\n",
        ")\n",
        "\n",
        "# Entrenar modelo\n",
        "print(\"üöÄ Iniciando entrenamiento...\")\n",
        "entrenador.train()\n",
        "\n",
        "# %% [7. Evaluaci√≥n del modelo]\n",
        "import numpy as np\n",
        "\n",
        "# Calcular perplexity (e^(p√©rdida))\n",
        "resultados_eval = entrenador.evaluate()\n",
        "perplexidad = np.exp(resultados_eval[\"eval_loss\"])\n",
        "print(f\"\\nüìä M√©tricas de evaluaci√≥n:\")\n",
        "print(f\"- P√©rdida en validaci√≥n: {resultados_eval['eval_loss']:.3f}\")\n",
        "print(f\"- Perplexity: {perplexidad:.3f}\")\n",
        "\n",
        "# %% [8. Guardar modelo]\n",
        "modelo.save_pretrained(model_save_path)\n",
        "tokenizer.save_pretrained(model_save_path)\n",
        "print(f\"\\nüíæ Modelo guardado en: {model_save_path}\")\n",
        "\n",
        "# %% [9. Interfaz de usuario profesional]\n",
        "from IPython.display import display\n",
        "import ipywidgets as widgets\n",
        "from transformers import pipeline\n",
        "import torch\n",
        "\n",
        "# Cargar modelo entrenado\n",
        "generador = pipeline(\n",
        "    'text-generation',\n",
        "    model=model_save_path,\n",
        "    tokenizer=model_save_path,\n",
        "    device=0 if torch.cuda.is_available() else -1,\n",
        "    framework='pt'\n",
        ")\n",
        "\n",
        "# Widgets interactivos\n",
        "estilo = {'description_width': '150px'}\n",
        "diseno = {'width': '500px', 'margin': '5px 0px'}\n",
        "\n",
        "menu_categorias = widgets.Dropdown(\n",
        "    options=[c.upper() for c in categorias],\n",
        "    description='üè∑Ô∏è Categor√≠a:',\n",
        "    style=estilo,\n",
        "    layout=diseno\n",
        ")\n",
        "\n",
        "control_longitud = widgets.IntSlider(\n",
        "    value=400,\n",
        "    min=200,\n",
        "    max=800,\n",
        "    step=50,\n",
        "    description='üìè Longitud:',\n",
        "    style=estilo,\n",
        "    layout=diseno\n",
        ")\n",
        "\n",
        "control_temperatura = widgets.FloatSlider(\n",
        "    value=0.72,\n",
        "    min=0.3,\n",
        "    max=1.2,\n",
        "    step=0.05,\n",
        "    description='üé≠ Creatividad:',\n",
        "    style=estilo,\n",
        "    layout=diseno\n",
        ")\n",
        "\n",
        "boton_generar = widgets.Button(\n",
        "    description='üöÄ Generar Noticia',\n",
        "    button_style='success',\n",
        "    layout={'width': '350px', 'margin': '15px 0px'}\n",
        ")\n",
        "\n",
        "area_salida = widgets.Output()\n",
        "\n",
        "# Funci√≥n de generaci√≥n mejorada\n",
        "def generar_noticia(_):\n",
        "    with area_salida:\n",
        "        area_salida.clear_output(wait=True)\n",
        "        prompt = f\"[{menu_categorias.value}]\\nFECHA: {datetime.now().strftime('%Y-%m-%d')}\\nTITULAR:\"\n",
        "\n",
        "        try:\n",
        "            generado = generador(\n",
        "                prompt,\n",
        "                max_length=control_longitud.value,\n",
        "                temperature=control_temperatura.value,\n",
        "                top_p=0.92,\n",
        "                repetition_penalty=1.35,\n",
        "                num_return_sequences=1,\n",
        "                no_repeat_ngram_size=2\n",
        "            )\n",
        "\n",
        "            texto_completo = generado[0]['generated_text']\n",
        "            contenido = texto_completo.split('CONTENIDO:', 1)[-1].strip()\n",
        "            parrafos = [p.strip() for p in contenido.split('\\n') if p.strip() and '---' not in p]\n",
        "\n",
        "            # Formatear salida profesional\n",
        "            print(\"üåê BBC NEWS - GENERACI√ìN AUTOM√ÅTICA\")\n",
        "            print(\"‚ïê\"*50)\n",
        "            print(f\"üîñ Categor√≠a: {menu_categorias.value}\")\n",
        "            print(f\"üìÖ Fecha: {datetime.now().strftime('%d/%m/%Y')}\")\n",
        "\n",
        "            if parrafos:\n",
        "                print(\"\\nüî• Titular Destacado:\")\n",
        "                print(f\"‚Ä¢ {parrafos[0]}\")\n",
        "\n",
        "                print(\"\\nüì∞ Cuerpo de la Noticia:\")\n",
        "                for parrafo in parrafos[1:]:\n",
        "                    if len(parrafo) > 30 and not parrafo.startswith(\"[\"):\n",
        "                        print(f\"\\n‚Ä¢ {parrafo}\")\n",
        "            else:\n",
        "                print(\"\\n‚ö†Ô∏è No se gener√≥ contenido v√°lido\")\n",
        "\n",
        "            print(\"\\n\" + \"‚ïê\"*50)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Error: {str(e)}\")\n",
        "\n",
        "boton_generar.on_click(generar_noticia)\n",
        "\n",
        "# Dise√±o final\n",
        "interfaz = widgets.VBox([\n",
        "    widgets.HTML(\"<h1 style='color:#1a73e8; font-family:Helvetica'>Generador BBC News v3.0</h1>\"),\n",
        "    widgets.HTML(\"<p style='font-family:Helvetica; color:#666'>Modelo: DistilGPT2 | Dataset: BBC News</p>\"),\n",
        "    widgets.HBox([menu_categorias, control_longitud]),\n",
        "    widgets.HBox([control_temperatura, boton_generar]),\n",
        "    area_salida\n",
        "])\n",
        "\n",
        "display(interfaz)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 762
        },
        "id": "SILoy6pF5Gg3",
        "outputId": "d92358c4-af23-4fbe-e724-24b90422f3f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/pip3\", line 4, in <module>\n",
            "    from pip._internal.cli.main import main\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/cli/main.py\", line 11, in <module>\n",
            "    from pip._internal.cli.autocompletion import autocomplete\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/cli/autocompletion.py\", line 10, in <module>\n",
            "    from pip._internal.cli.main_parser import create_main_parser\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/cli/main_parser.py\", line 9, in <module>\n",
            "    from pip._internal.build_env import get_runnable_pip\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/build_env.py\", line 19, in <module>\n",
            "    from pip._internal.cli.spinners import open_spinner\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/cli/spinners.py\", line 9, in <module>\n",
            "    from pip._internal.utils.logging import get_indentation\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/utils/logging.py\", line 29, in <module>\n",
            "    from pip._internal.utils.misc import ensure_dir\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/utils/misc.py\", line 42, in <module>\n",
            "    from pip._internal.exceptions import CommandError, ExternallyManagedEnvironment\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/exceptions.py\", line 315, in <module>\n",
            "    class InvalidWheelFilename(InstallationError):\n",
            "KeyboardInterrupt\n",
            "^C\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "MessageError",
          "evalue": "Error: credential propagation was unsuccessful",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mMessageError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-0d8b4bd705a6>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# %% [2. Montar Google Drive]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# %% [3. Configurar rutas]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m    101\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    135\u001b[0m   )\n\u001b[1;32m    136\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mephemeral\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m     _message.blocking_request(\n\u001b[0m\u001b[1;32m    138\u001b[0m         \u001b[0;34m'request_auth'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m         \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'authType'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'dfs_ephemeral'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mblocking_request\u001b[0;34m(request_type, request, timeout_sec, parent)\u001b[0m\n\u001b[1;32m    174\u001b[0m       \u001b[0mrequest_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpect_reply\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m   )\n\u001b[0;32m--> 176\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m    101\u001b[0m     ):\n\u001b[1;32m    102\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m'error'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mMessageError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mMessageError\u001b[0m: Error: credential propagation was unsuccessful"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Configuraci√≥n Inicial y Dependencias\n",
        "El c√≥digo comienza instalando versiones espec√≠ficas de las bibliotecas:\n",
        "\n",
        "Transformers (v4.51.3): Para cargar DistilGPT2 y sus utilidades de entrenamiento.\n",
        "\n",
        "Datasets (v2.14.4): Manejo eficiente del dataset BBC News.\n",
        "\n",
        "Torch (v2.6.0): Backend para aceleraci√≥n GPU.\n",
        "\n",
        "IPywidgets (v7.7.1): Interfaz interactiva en Colab.\n",
        "\n",
        "Prop√≥sito: Garantizar reproducibilidad y compatibilidad.\n",
        "\n",
        "2. Carga y Preprocesamiento de Datos\n",
        "Estructura del Dataset\n",
        "Los archivos .txt se organizan en carpetas por categor√≠a (business, tech, etc.).\n",
        "\n",
        "Cada archivo contiene:\n",
        "\n",
        "L√≠nea 1: T√≠tulo.\n",
        "\n",
        "L√≠neas 2+: Cuerpo del art√≠culo.\n",
        "\n",
        "Procesamiento Personalizado\n",
        "\n",
        "'content': ' '.join([linea.strip() for linea in contenido[1:] if linea.strip()])[:1800]\n",
        "Limpieza: Elimina espacios vac√≠os y saltos de l√≠nea.\n",
        "\n",
        "Truncamiento: Limita a 1800 caracteres para evitar desbordamiento de memoria.\n",
        "\n",
        "Formateo Estructurado\n",
        "\n",
        "[CATEGORIA]\n",
        "FECHA: 2025-05-15\n",
        "TITULAR: T√≠tulo del art√≠culo\n",
        "CONTENIDO: Texto completo...\n",
        "Objetivo: Crear prompts claros para guiar la generaci√≥n.\n",
        "\n",
        "3. Tokenizaci√≥n y Dataset\n",
        "Tokenizador Personalizado\n",
        "\n",
        "tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
        "[PAD]: Token a√±adido para rellenar textos cortos (necesario para batches).\n",
        "\n",
        "Longitud fija: 512 tokens (equilibrio entre contexto y memoria).\n",
        "\n",
        "Divisi√≥n Train/Validation\n",
        "\n",
        "dataset = dataset.train_test_split(test_size=0.1)\n",
        "90% entrenamiento, 10% validaci√≥n.\n",
        "\n",
        "Ventaja: Eval√∫a overfitting durante el entrenamiento.\n",
        "\n",
        "4. Entrenamiento del Modelo\n",
        "Hiperpar√°metros Clave\n",
        "Par√°metro\tValor\tExplicaci√≥n\n",
        "num_train_epochs\t10\tBalance entre costo y rendimiento\n",
        "per_device_train_batch_size\t4\tAjustado para GPU T4 (15GB RAM)\n",
        "learning_rate\t2e-5\tValor est√°ndar para fine-tuning\n",
        "eval_steps\t500\tEval√∫a cada 500 pasos\n",
        "Configuraci√≥n del Trainer\n",
        "\n",
        "data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
        "mlm=False: Entrenamiento por autoregresi√≥n (no masked language modeling).\n",
        "\n",
        "FP16: Activado para acelerar entrenamiento en GPU.\n",
        "\n",
        "5. Generaci√≥n de Texto\n",
        "Pipeline de Generaci√≥n\n",
        "\n",
        "generator = pipeline('text-generation', model=model_save_path, device=0)\n",
        "Par√°metros ajustables:\n",
        "\n",
        "temperature=0.72: Controla aleatoriedad (valores bajos = m√°s determinista).\n",
        "\n",
        "repetition_penalty=1.35: Reduce repeticiones de frases.\n",
        "\n",
        "no_repeat_ngram_size=2: Evita bigramas duplicados.\n",
        "\n",
        "Interfaz Interactiva\n",
        "Controles:\n",
        "\n",
        "Dropdown: Selecci√≥n de categor√≠a.\n",
        "\n",
        "Slider: Longitud (200-800 tokens) y creatividad.\n",
        "\n",
        "Salida Estructurada:\n",
        "\n",
        "\n",
        " BBC NEWS - GENERACI√ìN AUTOM√ÅTICA\n",
        " Categor√≠a: TECH\n",
        " Fecha: 15/05/2025\n",
        " Titular: OpenAI lanza nuevo modelo...\n",
        "6. Evaluaci√≥n y M√©tricas\n",
        "C√°lculo de Perplexity\n",
        "python\n",
        "perplexity = np.exp(eval_results[\"eval_loss\"])\n",
        "Interpretaci√≥n:\n",
        "\n",
        "10.43: Indica que el modelo est√° relativamente seguro de sus predicciones.\n",
        "\n",
        "30: Sugerir√≠a problemas de coherencia.\n",
        "\n",
        "Ejemplo de Generaci√≥n\n",
        "\n",
        "[TECH]\n",
        "FECHA: 2025-05-15\n",
        "TITULAR: DeepMind anuncia avance en AGI\n",
        "CONTENIDO: El nuevo sistema Alpha-X muestra capacidades...\n",
        "Fortalezas: Coherencia tem√°tica y estructura period√≠stica.\n",
        "\n",
        "Debilidades: Fechas y nombres inventados (limitaci√≥n conocida de LLMs)."
      ],
      "metadata": {
        "id": "RVCVjeQHsz1A"
      }
    }
  ]
}